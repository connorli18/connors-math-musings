<div class='section'>AI Cheat Sheet - Final Exam</div><div class='subsection'>Adversarial Search</div><b>Definition:</b> Adversarial search pertains to <u>deterministic</u>, <u>full-observable</u>, <u>zero-sum</u>, <u>two agents acting alternately</u>.<br><b>Formalization:</b> [1] Initial State [2] Player(s): defines which player has move in any state [3] Actions: set of legal moves in any state [4] Transition function: $S \times A \rightarrow S$ states and actions map to new states [5] Terminal Test: returns True is game is over, False otherwise, defines terminal states [6] Utility function$(s,p)$ (objective function): assigns values to wins, loss, draw $(+1, -1, 0)$ for state $s$ and player $p$.<div class='subsubsection'>Minimax</div><b>Properties:</b> [1] DFS time: $O(b^m)$ [2] DFS Space: $O(bm)$ [3] Limited resources time $\implies$ can't search leaves<br><b>Optimal Strategy:</b> [1] DFS of game tree [2] optimal leaf node at any depth [3] both players play optimally (alternate min and max) [4] Propogate minimax values up the tree once terminal nodes are found [5] Start with Max(start state).<div class="center-it"><code class="lstlisting" style="text-decoration: none !important; color: white !important;">&nbsp;&nbsp;&nbsp;&nbsp;#ignore&nbsp;a,b&nbsp;for&nbsp;non&nbsp;pruning<br>&nbsp;&nbsp;&nbsp;&nbsp;function&nbsp;MAX(state,a,b)<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;returns&nbsp;TUPLE&nbsp;of&nbsp;&lt;State,Util&gt;:<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;TERMINAL-TEST(state):<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;&lt;NULL,&nbsp;EVAL(state)&gt;<br><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;maxChild,maxUtil&gt;&nbsp;=&nbsp;&lt;NULL,-inf&gt;<br><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for&nbsp;child&nbsp;in&nbsp;state.children():<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;_,util&gt;&nbsp;=&nbsp;MIN(child,a,b)<br><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;util&nbsp;&lt;&nbsp;maxUtil:<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;maxChild,maxUtil&gt;&nbsp;=&nbsp;&lt;child,Util&gt;<br><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;maxUtil&nbsp;&gt;=&nbsp;b:<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;break<br><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;maxUtil&nbsp;&gt;&nbsp;a:<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;b&nbsp;=&nbsp;maxUtil<br><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;&lt;maxChild,maxUtil&gt;<br><br>&nbsp;&nbsp;&nbsp;&nbsp;#ignore&nbsp;a,b&nbsp;for&nbsp;non&nbsp;pruning<br>&nbsp;&nbsp;&nbsp;&nbsp;function&nbsp;MIN(state,a,b)<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;returns&nbsp;TUPLE&nbsp;of&nbsp;&lt;State,Util&gt;:<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;TERMINAL-TEST(state):<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;&lt;NULL,&nbsp;EVAL(state)&gt;<br><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;minChild,minUtil&gt;&nbsp;=&nbsp;&lt;NULL,+inf&gt;<br><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for&nbsp;child&nbsp;in&nbsp;state.children():<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;_,util&gt;&nbsp;=&nbsp;MAX(child,a,b)<br><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;util&nbsp;&lt;&nbsp;minUtil:<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;minChild,minUtil&gt;&nbsp;=&nbsp;&lt;child,Util&gt;<br><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;minUtil&nbsp;&lt;=&nbsp;a:<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;break<br><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;minUtil&nbsp;&lt;&nbsp;b:<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;b&nbsp;=&nbsp;minUtil<br><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;&lt;minChild,minUtil&gt;<br></code></div><b>Limited Resources:</b> [1] Replace terminal utilites with evaluation function [2] Use IDS [3] Use $\alpha, \beta$ pruning where $\alpha \geq \beta$<div class="itemize"><ul><li> $\alpha (-\infty)$: largest value for Max across seen children (current lower bound on Max), update only at Max nodes<br><li> $[2] \;\beta (+\infty)$: lowest value for Min across seen children (current upper bound on Min), update only at Min nodes<br></ul></div><b>Move Ordering:</b> Examine first successors that are the best. Worst ordering: $O(b^m)$, Ideal ordering: $O(b^{m/2})$. Find best nodes by [1] remembering best moves, [2] using domain knowledge [3] bookkeeping stats for repetition.<br><br><b>Expectiminimax (EM):</b> Generalizes Minimax to handle chance nodes.$$\text{EM}(s) = \begin{cases}\text{Utility}(s) &\text{if Terminal-test}(s)\\\max_{a \in \text{Actions}} \text{EM}(\text{Result}(s,a)) &\text{if Player}(s) = \text{Max}\\\min_{a \in \text{Actions}} \text{EM}(\text{Result}(s,a)) &\text{if Player}(s) = \text{Min}\\\sum_r P(r) \cdot \text{EM}(\text{Result}(s,r)) &\text{if Player}(s) = \text{Chance}\end{cases}$$<div class='subsection'>Knowledge Based Agents</div><b>Definition:</b> Knowledge-Based Agents are composed of [1] knowledge base: domain specific content (set of sentences, assertions about world) [2] Inference mechanism: domain-independent algos<br><br><b>Declarative Approach:</b> [1] Add new sentences: tell it what it needs to know [2] Query what is known: ask itself what to do, answers should follow knowledge base (KB)<div class="itemize"><ul><li> [1] <u>Atomic Proposition</u>: a statement that can either be true of false, only one uppercase letter ("5 is prime")<br><li> [2] <u>Compound Propositions</u>: constructed from atomic propositions and uses logical connections, parentheses, etc.<br><li> [3] <u>Implications</u> ($p \rightarrow q$): [1] Converse $q \rightarrow p$ [2] Contrapositive $\not q \rightarrow p$ [3] Inverse $\not p \rightarrow q$<br><li> [4] <u>Syntax</u>: formal structure of sentences<br><li> [5] <u>Semantic</u>: truth of sentences with reference to models<br></ul></div><div class='subsection'>PL & Inference Rules</div>

<div style="display: flex; justify-content: center; padding: 15px;">
    <img src="https://media.cheggcdn.com/media/041/041855fd-d599-4d12-95b6-4e0f96a73108/php5p1A6M.png" alt="Image" style="width: 500px; border: 5px solid black;">
</div>


<div style="display: flex; justify-content: center; padding: 15px;">
    <img src="https://www.poly-ed.com/wp-content/uploads/2020/01/rulesofinference-768x494.png" alt="Image" style="width: 500px; border: 5px solid black;">
</div>


<b>Entailment and Inference:</b> [1] Syntax $KB \vdash \alpha$: determine entailment by <u>theorem proving</u>, build proof of $\alpha$ without enumerating and checking all models [2] Semantics $KB \vDash \alpha$: determine entailment by <u>model checking</u>, enumerate all models (truth tables) and show sentence $\alpha$ holds in all models, exponential time.<br><br><b>Soundness and Completeness:</b> [1] Sound: does not infer false formulas, only derives entailed sentences [2] Complete: derives all entailed sentences [3] Valid: a sentence is valid if its true in all models, deduction theorem: $KB \vDash a \iff (KB \rightarrow a)$ [4] Satisfiable: if it is true in some model [5] Unsatisfiable: if it is true in no models, $KB \vDash a \iff (KB \wedge \neg a)$.<br><br><b>Theorem Proving:</b> Two ways ensure completeness, [1] Proof by resolution (resolution rule) [2] Forward or Backward Chaining: modus ponens on propositions (Horn clauses) [3] CNF: conjunction ($\wedge$) of disjunction ($\vee$) of literals<div class="center-it"><code class="lstlisting" style="text-decoration: none !important; color: white !important;"><br>&nbsp;&nbsp;&nbsp;&nbsp;function&nbsp;PL-Resolution(KB,&nbsp;a)&nbsp;returns&nbsp;True&nbsp;or&nbsp;False<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;inputs:&nbsp;KB&nbsp;(knowledge&nbsp;base),a&nbsp;(sentence)<br><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;clauses&nbsp;#set&nbsp;of&nbsp;CNF&nbsp;representation&nbsp;of&nbsp;KB&nbsp;and&nbsp;(not&nbsp;a)<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;new&nbsp;#empty&nbsp;set<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;loop&nbsp;do<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for&nbsp;each&nbsp;Ci,&nbsp;Cj&nbsp;in&nbsp;clauses&nbsp;do<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;resolvents&nbsp;&lt;--&nbsp;PL-Resolve(Ci,&nbsp;Cj)<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;resolvents&nbsp;contains&nbsp;empty&nbsp;clause,&nbsp;return&nbsp;True<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;new&nbsp;&lt;--&nbsp;new&nbsp;+&nbsp;resolvents<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;new&nbsp;subset&nbsp;clauses,&nbsp;return&nbsp;False<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;clauses&nbsp;&lt;--&nbsp;clauses&nbsp;+&nbsp;new<br></code></div>[1] Inference in PL with Horn clauses (exactly one positive literal) is sound & complete<br><br>$[2]$ Limits of PL: not expressive enough to describe world around us, not compact enough so can't express a fact without enumerating objects<br><br>$[3]$ FOL is alternative: inference is more complex (but possible), expressiveness makes it more attuned to natural language<br><br><b>Why IDS:</b> [1] Leaves are too expensive to search, use evaluation measure, accuracy increases with search depth [2] If time is up, player can use previous level as move [3] Node ordering can help reduce stuff with $\alpha, \beta$ pruning.<br><br><b>Proof by Resolution:</b> Assume $KB \vDash \neg\alpha$ and show that it leads to a contradiction. Thus, we know that $\alpha \in KB$.

<div style="display: flex; justify-content: center; padding: 15px;">
    <img src="https://i.postimg.cc/jq89nJhh/Screen-Shot-2023-11-15-at-11-44-37-AM.png" alt="Image" style="width: 500px; border: 5px solid black;">
</div>


<div class='subsection'>Machine Learning</div><b>Definition:</b> Building programs that learn from experience, without specifying the rules to solve the problem at hand. <u>Supervised:</u> learning with labeled data, <u>Unsupervised:</u> learning with unlabeled data.<br><b>Unsupervised:</b> $f: \mathbb{R}^d \rightarrow \{C_1,\dots,C_k\}$, find clusters in a population.<br><b>Supervised:</b> Take some $y_i \in \{-1,+1\}$. Classifier function $f: \mathbb{R}^d \rightarrow \{-1,+1\}$ defines decision boundary. Regressor function $f: \mathbb{R}^d \rightarrow \mathbb{R}$, $y \in \mathbb{R}$. Examples of regression: income as a function of age, weight of fruit by length, etc.<br>$$\textbf{Training Error:}\;E^{\text{train}}(f) = \sum_{i=1}^n \text{loss}(y_i,f(\vec{x}_i)) \quad \quad \textbf{Classification Error:}\;\text{loss}(y_i,f(\vec{x}_i)) = \begin{cases}1 &\text{if sign}(y_i) \neq \text{sign}(f(\vec{x}_i))\\0 &\text{otherwise}\end{cases}$$$$\textbf{Least Squares Loss:}\;\text{loss}(y_i,f(\vec{x}_i)) = (y-f(\vec{x}_i))^2$$<b>Underfitting vs. Overfitting:</b> High bias (underfitting), high variance (overfitting), balancing is key to ML<div class="itemize"><ul><li> [1] Use simple models to avoid overfitting<br><li> [2] Reduce the number of features manually or do feature selection<br><li> [3] Do model selection, pick out of a list of models based on performance<br><li> [4] Use regularization, keep features but reduce their importance by setting coefficients $\rightarrow 0$<br><li> [5] Do cross-validation to estimate test-error, resampling method on iterated datasets<br></ul></div><b>Train, Validation, Test:</b> Split the model into $3$ portions, training, validation, and test to see accuracy<br><b>Class Imbalance:</b> Affects majority voting, if most examples belong in one class. Down sampling or up sampling can address this problem.<br><br><b>K-Fold Cross Validation:</b> <br>$[1]$ Randomly partition $D$ into $k$ equal subsets.<br>$[2]$ For $j = 1$ to $k$<br><div class="itemize"><ul><li> Train algo $A$ on $D_i \;\forall i \wedge i \neq j$ and get $f_j$<br><li> Apply $f_j$ to $D_j$ and compute $E^{D_j}$<br></ul></div>$[3]$ Average errors over all folds $\sum_{j=1}^k E^{D_j}$

<div style="display: flex; justify-content: center; padding: 15px;">
    <img src="https://i.postimg.cc/zfS2zLBK/Screen-Shot-2023-11-15-at-12-15-16-PM.png" alt="Image" style="width: 500px; border: 5px solid black;">
</div>


<div style="display: flex; justify-content: center; padding: 15px;">
    <img src="https://i.postimg.cc/zGT07LT6/Screen-Shot-2023-11-15-at-12-16-01-PM.png" alt="Image" style="width: 500px; border: 5px solid black;">
</div>

<div class='subsection'>K Nearest Neighbor</div><b>Metric Norms:</b> $p=1:$ Manhattan distance, $p=2:$ Euclidean distance<br><b>Terminology:</b> [1] $N_k(\vec{x}_q)$: set of $k$ nearest neighbors of $\vec{x}_q$ [2] Majority voting: label with the most votes wins!<br><b>Algorithm:</b> [<u>Training</u>] Add each example $(\vec{x},y)$ to dataset $D$ where $y \in \{-1,+1\}[<u>Classification</u>] Take some example $\vec{x}_q$ that needs to be classified. Find $N_k(\vec{x}_q)$. We classify $\hat{y}_q = \text{sign}(\sum_{\vec{x}_i \in N_k} y_i)$ <br><br><b>Cross-Validation:</b> Use CV to select $k \in [k_{\min},k_{\max}]$ (number of NN). <br>Divide training set into $5$-$10$ folds.<div class="center-it"><code class="lstlisting" style="text-decoration: none !important; color: white !important;">For&nbsp;kcurr&nbsp;in&nbsp;[kmin,&nbsp;kmax]:<br>&nbsp;&nbsp;&nbsp;&nbsp;For&nbsp;j&nbsp;=&nbsp;1&nbsp;to&nbsp;5:&nbsp;#depends&nbsp;on&nbsp;folds<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Use&nbsp;Di&nbsp;where&nbsp;i&nbsp;=/=&nbsp;j&nbsp;as&nbsp;training&nbsp;data<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Apply&nbsp;KNN&nbsp;with&nbsp;kcurr&nbsp;to&nbsp;calculate&nbsp;E(Dj)<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;E(Dj)&nbsp;=&nbsp;sum_{xi&nbsp;in&nbsp;Dj}&nbsp;loss(yi,&nbsp;predict_yi)<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>&nbsp;&nbsp;&nbsp;&nbsp;Average&nbsp;error&nbsp;over&nbsp;all&nbsp;folds<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;CVk&nbsp;=&nbsp;1/5&nbsp;*&nbsp;(E(D1)&nbsp;+&nbsp;...&nbsp;+&nbsp;E(D5))<br><br>k&nbsp;=&nbsp;argmin(CVk)&nbsp;#select&nbsp;smallest&nbsp;k&nbsp;if&nbsp;tied<br></code></div><b>Pros of NN</b>: [1] Simple to implement [2] Works well [3] No models, assumptions, parameters [4] Can be extended easily <br><br><b>Cons of NN:</b> [1] Requires large space to store training set [2] Slow, with $n$ examples and $d$ features, model takes $O(nd)$ [3] Raw features to measure "closeness" is not always best [4] Curse of dimensionality (Euclidean distnace not useful in higher dimensions)<br><br><b>Issues:</b> Efficiency, Effectiveness<br><br><b>Solutions to KNN:</b> [1] Remove duplicates, fast DS, reduce dimension [2] Random projections, hashing, kd-trees, [3] Normalize and weight features [4] Don't keep data points, but keep labeled areas <br><br><b>Curse of Dimensionality:</b> peaking phenomena, with a fixed number of training data, the predictive power of a classifier/regressor first increases with increasing features, then it decreases.<div class='subsection'>Decision Trees</div><b>Measure of Homogeneity:</b> Entropy$(S) = \sum_{i=1}^c -p_i \log_2 p_i$, Gini$(S) = \sum_{i=1}^c 1-\sum_{i=1}^c p_i^2$, lower entropy $=$ higher purity!<br><b>Information Gain:</b> $\text{Gain}(S,A) = \text{Entropy}(S) - \sum_{v \in \text{Values}(A)} \frac{|S_v|}{|S|} \text{Entropy}(S_v)$, higher gain is more preferable.<br><b>Numerical Features:</b> Discretize on the fly. Order the $k$ values and determine the best out of the $k-1$ points to pick the best split. Find it via testing the discretization against past gains.<br><b>Pruning Strategies:</b> [1] Stop growing tree before it gets too complicated [2] Grow a tree then cut back: remove a subtree if performance of the new tree is with $\epsilon$ of old tree, post-pruning also another strategy<br><b>Practical Considerations:</b> [1] Dimensionality reduction [2] use ensemble methods (random forest) [3] balance data before training: undersampling, reducing majority class & oversampling, increasing minority class (SMOTE, ADASYN)<br><b>Regression Trees:</b> Splitting criteria for regression tree: choose the feature that will lead to the smallest sum of squares. When pruning the tree, use mean square error.

<div style="display: flex; justify-content: center; padding: 15px;">
    <img src="https://i.postimg.cc/tJLNtyJ9/Screen-Shot-2023-11-15-at-12-47-41-PM.png" alt="Image" style="width: 500px; border: 5px solid black;">
</div>

<b>Linear Regression:</b> Minimize risk function. For multiple features, use gradient descent for optimization.\begin{align*}\textbf{Lin Reg:}&\;f(\vec{x}_i) = \beta_0 + \sum_{j=1}^d \beta_jx_{ij}\\\textbf{Least Square Loss:}&\;\text{loss}(y_i,f(\vec{x}_i)) = (y_i-f(\vec{x}_i))^2\\\textbf{Risk:}&\;R = \frac{1}{2n} \sum_{i=1}^n  (y_i-f(\vec{x}_i))^2\\\beta_1 &= \frac{\sum_{i=1}^{n} y_ix_i - \frac{1}{n} \sum_{i=1}^{n} y_i \sum_{i=1}^{n} x_i}{\sum_{i=1}^{n} x_i^2 - \frac{1}{n} \sum_{i=1}^{n} x_i \sum_{i=1}^{n} x_i}\\\beta_0 &= \frac{1}{n} \sum_{i=1}^n y_i - \frac{\beta_1}{n} \sum_{i=1}^{n} x_i\\\beta'_i &= \beta_i - \alpha \frac{\partial}{\partial \beta_1} R(\beta_0,\beta_1)\end{align*}<b>Practical Considerations (LR):</b> [1] Scaling $x_i := \frac{x_i - \mu_i}{\text{stdev}(x_i)}$ [2] Learning rate (too big/too small) [3] $R$ should decrease after each iteration [4] Declare convergence if $|R_i - R_{i+1}| < \epsilon$.<div class='subsection'>Neural Networks</div><b>Deep Learning:</b> using a neural network with a series of hidden layers of non-linear operations between input and output.<br><b>Perceptron:</b> Linear classification method, simplest NN, for perfectly separable data. Defined by $f(\vec{x}_i) = \text{sign}\left(\sum_{j=0}^{d} w_jx_{ij}\right)$.<br><div class="center-it"><code class="lstlisting" style="text-decoration: none !important; color: white !important;">Input,&nbsp;set&nbsp;of&nbsp;examples:&nbsp;(x1,y1)...(xn,&nbsp;yn)<br>Output,&nbsp;perceptron&nbsp;defined&nbsp;by&nbsp;w1,w2...wn<br><br>1.&nbsp;Initialize&nbsp;weights&nbsp;wj&nbsp;=&nbsp;0<br>2.&nbsp;Repeat&nbsp;until&nbsp;convergence<br>&nbsp;&nbsp;&nbsp;&nbsp;For&nbsp;each&nbsp;example&nbsp;xi&nbsp;for&nbsp;i&nbsp;=&nbsp;{1...n}<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;yi*f(xi)&nbsp;&lt;=&nbsp;0<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;update&nbsp;wj&nbsp;--&gt;&nbsp;wj:=&nbsp;wj&nbsp;+&nbsp;a*yi*xij&nbsp;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#where&nbsp;a&nbsp;=&nbsp;learning&nbsp;rate<br></code></div><b>XOR Perceptron (sigmoid):</b>


<div style="display: flex; justify-content: center; padding: 15px;">
    <img src="https://i.postimg.cc/qBYwdcLW/Screen-Shot-2023-11-15-at-3-38-02-PM.png" alt="Image" style="width: 500px; border: 5px solid black;">
</div>


<b>Non-Linear functions:</b> Use sigmoid function. $f(x_i) = \frac{1}{1+e^{-\sum_{j=0}^d w_jx_{ij}}}$. Allows for gradient descent, to model XOR.<br><b>Perceptron Example.</b> Using the table, [1] run algorithm [2] find decision function [3] slope/intercept.<br><br>


<div style="display: flex; justify-content: center; align-items: center;">
        <table style="border-collapse: separate; border-spacing: 10px; background-color: transparent;">
            <tr>
                <th style="border: 1px solid #ffffff; padding: 10px; text-align: center;">\( x_1 \)</th>
                <th style="border: 1px solid #ffffff; padding: 10px; text-align: center;">\( x_2 \)</th>
                <th style="border: 1px solid #ffffff; padding: 10px; text-align: center;">\( y \)</th>
            </tr>
            <tr>
                <td style="border: 1px solid #ffffff; padding: 10px; text-align: center;">\( 1 \)</td>
                <td style="border: 1px solid #ffffff; padding: 10px; text-align: center;">\( 2 \)</td>
                <td style="border: 1px solid #ffffff; padding: 10px; text-align: center;">\( +1 \)</td>
            </tr>
            <tr>
                <td style="border: 1px solid #ffffff; padding: 10px; text-align: center;">\( 2 \)</td>
                <td style="border: 1px solid #ffffff; padding: 10px; text-align: center;">\( 1 \)</td>
                <td style="border: 1px solid #ffffff; padding: 10px; text-align: center;">\( +1 \)</td>
            </tr>
            <tr>
                <td style="border: 1px solid #ffffff; padding: 10px; text-align: center;">\( -1 \)</td>
                <td style="border: 1px solid #ffffff; padding: 10px; text-align: center;">\( -1 \)</td>
                <td style="border: 1px solid #ffffff; padding: 10px; text-align: center;">\( -1 \)</td>
            </tr>
            <tr>
                <td style="border: 1px solid #ffffff; padding: 10px; text-align: center;">\( -1 \)</td>
                <td style="border: 1px solid #ffffff; padding: 10px; text-align: center;">\( 1 \)</td>
                <td style="border: 1px solid #ffffff; padding: 10px; text-align: center;">\( -1 \)</td>
            </tr>
        </table>
    </div>

<br><br>[1] $\vec{w} = \langle 0,0,0\rangle \implies y_1f(\vec{x}) = 1 \cdot \vec{w} \cdot \langle 1,2,1\rangle = 0 \leq 0$<br>$[$Adjust$]\;\vec{w} = \vec{w} + 1 \cdot \langle 1,2,1\rangle = \langle 1,2,1\rangle$ <br>$[2]\;\vec{w} = \langle 1,2,1\rangle \implies y_2f(\vec{x}) = 1 \cdot \vec{w} \cdot \langle 2,1,1\rangle = 5 \nleq 0$<br>$[3]\;\vec{w} = \langle 1,2,1\rangle \implies y_3f(\vec{x}) = -1 \cdot \vec{w} \cdot \langle -1,-1,-1\rangle = 4 \nleq 0$<br>$[4]\;\vec{w} = \langle 1,2,1\rangle \implies y_4f(\vec{x}) = -1 \cdot \vec{w} \cdot \langle -1,1,-1\rangle = 0 \leq 0$ <br>$[$Adjust$] \vec{w} = \vec{w} + (-1) \cdot \langle -1,1,-1\rangle = \langle 2,1,2 \rangle$<br><br>[1] $\vec{w} = \langle 1,2,1 \rangle \implies y_1f(\vec{x}) = 6 \nleq 0$<br>$[2]$ $\vec{w} = \langle 1,2,1 \rangle \implies y_2f(\vec{x}) = 7\nleq 0$<br>$[3]$ $\vec{w} = \langle 1,2,1 \rangle \implies y_3f(\vec{x}) = 5\nleq 0$<br>$[4]$ $\vec{w} = \langle 1,2,1 \rangle \implies y_4f(\vec{x}) = 3\nleq 0$